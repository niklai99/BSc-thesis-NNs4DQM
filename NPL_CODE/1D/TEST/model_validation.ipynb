{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunParameters:\n",
    "    '''classe che gestisce i parametri della run'''\n",
    "    \n",
    "    def __init__(self, out_dir: str, ndir=0, ntoy=100):\n",
    "        '''init: imposto il nome della cartella di output, genero il nome della cartella contenente il file e cerco il file'''\n",
    "        \n",
    "        self.out_dir = out_dir\n",
    "        self.folder_name = os.listdir(self.out_dir+'/0/')[ndir]\n",
    "        self.fname = [name for name in os.listdir(self.out_dir+'/0/'+self.folder_name) if f'_toy{ntoy}_t.txt' in name][0]\n",
    "        print('\\nFolder name: ' + self.folder_name)\n",
    "        print('File name: ' + self.fname)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fetch_parameters(self) -> list:\n",
    "        '''uso il nome del file e della cartella per estrarre i parametri della run'''\n",
    "        \n",
    "        self.check_point_t = int(self.fname.split(\"patience\", 1)[1].split(\"_\", 1)[0])\n",
    "        self.toys = int(self.fname.split(\"toy\", 1)[1].split(\"_\", 1)[0])\n",
    "        self.ref = (self.fname.split(\"_\")[2]).split(\"ref\")[1]\n",
    "        self.bkg = (self.fname.split(\"_\")[3]).split(\"bkg\")[1]\n",
    "        self.sig = (self.fname.split(\"_\")[4]).split(\"sig\")[1]\n",
    "        self.w_clip = self.folder_name.split('wclip',1)[1]\n",
    "        self.latent = int(self.folder_name.split(\"latent\", 1)[1].split(\"_\", 1)[0])\n",
    "        self.layers = int(self.folder_name.split(\"layers\", 1)[1].split(\"_\", 1)[0])\n",
    "        self.epochs = int(self.folder_name.split('epochs',1)[1].split('_')[0])\n",
    "        \n",
    "        self.parameters = [self.toys, self.w_clip, self.epochs, self.check_point_t, self.ref, self.bkg, self.sig, self.latent, self.layers]\n",
    "        \n",
    "        return self.parameters\n",
    "    \n",
    "    def print_parameters(self):\n",
    "        '''stampo i parametri per controllare corrispondano alla run'''\n",
    "        \n",
    "        print(f'\\nParameters:                                              \\\n",
    "                        \\n Toys:          {self.toys}                      \\\n",
    "                        \\n Latent space:  {self.latent}                    \\\n",
    "                        \\n Layers:        {self.layers}                    \\\n",
    "                        \\n W_clip:        {self.w_clip}                    \\\n",
    "                        \\n Epochs:        {self.epochs}                    \\\n",
    "                        \\n Check_point_t: {self.check_point_t}             \\\n",
    "                        \\n Ref, Bkg, Sig: {self.ref} {self.bkg} {self.sig} \\n'\n",
    "             )\n",
    "        \n",
    "        return  \n",
    "    \n",
    "    def fetch_file(self) -> str:\n",
    "        '''genero il nome completo del file contenente il t finale'''\n",
    "        \n",
    "        self.tfile = f'1D_patience{self.check_point_t}_ref{self.ref}_bkg{self.bkg}_sig{self.sig}\\\n",
    "        _epochs{self.epochs}_latent{self.latent}_layers{self.layers}_wclip{self.w_clip}\\\n",
    "        /1D_patience{self.check_point_t}_ref{self.ref}_bkg{self.bkg}_sig{self.sig}_toy{self.toys}_t.txt'\n",
    "        \n",
    "        self.tfile = self.tfile.replace(' ', '')\n",
    "        \n",
    "        return self.tfile\n",
    "    \n",
    "    def fetch_history(self) -> str:\n",
    "        '''genero il nome completo del file contenente il t per ogni checkpoint'''\n",
    "        \n",
    "        self.thistory = f'1D_patience{self.check_point_t}_ref{self.ref}_bkg{self.bkg}_sig{self.sig}\\\n",
    "        _epochs{self.epochs}_latent{self.latent}_layers{self.layers}_wclip{self.w_clip}\\\n",
    "        /1D_patience{self.check_point_t}_ref{self.ref}_bkg{self.bkg}_sig{self.sig}_toy{self.toys}_history{self.check_point_t}.h5'\n",
    "        \n",
    "        self.thistory = self.thistory.replace(' ', '')\n",
    "        \n",
    "        return self.thistory\n",
    "    \n",
    "    def fetch_model(self):\n",
    "        '''genero il nome completo del file contenente il modello'''\n",
    "        r = random.randrange(self.toys)\n",
    "        self.model = [name for name in os.listdir(self.out_dir+f'/{r}/'+self.folder_name) if '_model.json' in name][0]\n",
    "        self.weights = [name for name in os.listdir(self.out_dir+f'/{r}/'+self.folder_name) if '_weights.h5' in name][0]\n",
    "        \n",
    "        model_path = self.out_dir+f'/{r}/'+self.folder_name+'/'+self.model\n",
    "        weights_path = self.out_dir+f'/{r}/'+self.folder_name+'/'+self.weights\n",
    "        \n",
    "        return model_path, weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(yTrue, yPred):\n",
    "    return K.sum(-1.*yTrue*(yPred) + (1-yTrue)*N_D/float(N_R)*(K.exp(yPred)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSample(FILE_NAME, n):\n",
    "     \n",
    "    df=pd.read_csv(FILE_NAME)\n",
    "\n",
    "    print('Building pandas!')\n",
    "    \n",
    "    df=df.sample(n=n)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder name: 1D_patience1000_ref40000_bkg4000_sig0_epochs200000_latent3_layers1_wclip7.0\n",
      "File name: 1D_patience1000_ref40000_bkg4000_sig0_toy100_t.txt\n",
      "\n",
      "Parameters:                                                                      \n",
      " Toys:          100                                              \n",
      " Latent space:  3                                            \n",
      " Layers:        1                                            \n",
      " W_clip:        7.0                                            \n",
      " Epochs:        200000                                            \n",
      " Check_point_t: 1000                                     \n",
      " Ref, Bkg, Sig: 40000 4000 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "WCLIP = 7 # MODIFY THIS FOR CHANGING DIRECTORY\n",
    "NTOYS = 100\n",
    "\n",
    "OUT_PATH = f'E200kW{WCLIP}'\n",
    "DOF = 10\n",
    "NFOLDER = 0\n",
    "\n",
    "rPar = RunParameters(OUT_PATH, NFOLDER, NTOYS)\n",
    "toys, w_clip, epochs, check_point_t, ref, bkg, sig, latent, layers = rPar.fetch_parameters()\n",
    "rPar.print_parameters()\n",
    "\n",
    "model, weights = rPar.fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pandas!\n",
      "Reference DF shape:\n",
      "(40000, 1)\n",
      "Building pandas!\n",
      "Data DF shape:\n",
      "(12000, 1)\n",
      "target shape:\n",
      "(52000, 1)\n",
      "feature shape:\n",
      "(52000, 2)\n",
      "feature shape: (52000, 2)\n"
     ]
    }
   ],
   "source": [
    "SIG_NAME = '/lustre/cmswork/nlai/NPL_CODE/1D/TEST/sig.txt'\n",
    "BKG_NAME = '/lustre/cmswork/nlai/NPL_CODE/1D/TEST/bkg.txt'\n",
    "DATA_NAME = '/lustre/cmswork/nlai/NPL_CODE/1D/TEST/sig_bkg.txt'\n",
    "REF_NAME = '/lustre/cmswork/nlai/NPL_CODE/1D/TEST/ref.txt'\n",
    "\n",
    "N_Sig = 2000\n",
    "N_Bkg = 10000\n",
    "N_Ref = 40000\n",
    "\n",
    "N_D = N_Sig + N_Bkg\n",
    "N_R = N_Ref\n",
    "\n",
    "# SIGNAL POISSON FLUCTUATIONS\n",
    "# N_Sig_P = np.random.poisson(lam=N_Sig, size=1)\n",
    "# N_Sig_p = N_Sig_P[0]\n",
    "# print('N_Sig: '+str(N_Sig))\n",
    "# print('N_Sig_Pois: '+str(N_Sig_p))\n",
    "\n",
    "# # BACKGROUND POISSON FLUCTUATIONS\n",
    "# N_Bkg_P = np.random.poisson(lam=N_Bkg, size=1)\n",
    "# N_Bkg_p = N_Bkg_P[0]\n",
    "# print('N_Bkg: '+str(N_Bkg))\n",
    "# print('N_Bkg_Pois: '+str(N_Bkg_p))\n",
    "\n",
    "\n",
    "# REFERENCE\n",
    "HLF_REF = BuildSample(FILE_NAME=REF_NAME, n=N_Ref)\n",
    "print('Reference DF shape:')\n",
    "print(HLF_REF.shape)\n",
    "\n",
    "# # BACKGROUND\n",
    "# HLF_BKG = BuildSample(FILE_NAME=BKG_NAME, n=N_Bkg)\n",
    "# print('Background DF shape:')\n",
    "# print(HLF_REF.shape)\n",
    "\n",
    "# # SIGNAL\n",
    "# HLF_SIG = BuildSample(FILE_NAME=SIG_NAME, n=N_Sig)\n",
    "# print('Signal DF shape:')\n",
    "# print(HLF_SIG.shape)\n",
    "\n",
    "# DATA\n",
    "HLF_DATA = BuildSample(FILE_NAME=DATA_NAME, n=N_Sig+N_Bkg)\n",
    "print('Data DF shape:')\n",
    "print(HLF_DATA.shape)\n",
    "\n",
    "#TARGETS\n",
    "target_REF = np.zeros(N_Ref)\n",
    "target_DATA = np.ones(N_Sig+N_Bkg)\n",
    "target = np.append(target_REF, target_DATA)\n",
    "target = np.expand_dims(target, axis=1)\n",
    "print('target shape:')\n",
    "print(target.shape)\n",
    "\n",
    "feature = np.concatenate((HLF_REF, HLF_DATA), axis=0)\n",
    "feature = np.concatenate((feature, target), axis=1)\n",
    "print('feature shape:')\n",
    "print(feature.shape)\n",
    "\n",
    "np.random.shuffle(feature)\n",
    "print(f'feature shape: {feature.shape}')\n",
    "target = feature[:, -1]\n",
    "feature = feature[:, :-1]\n",
    "\n",
    "for j in range(feature.shape[1]):\n",
    "    vec = feature[:, j]\n",
    "    mean = np.mean(vec)\n",
    "    std = np.std(vec)\n",
    "    if np.min(vec) < 0:\n",
    "        vec = vec-mean\n",
    "        vec = vec/std\n",
    "    elif np.max(vec) > 1.0:\n",
    "        vec = vec *1./ mean\n",
    "    feature[:, j] = vec\n",
    "    \n",
    "batch_size=feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final t_OBS = -345.65863\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open(model, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(weights)\n",
    "# print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss = Loss,  optimizer = 'adam')\n",
    "score = loaded_model.evaluate(feature, target, batch_size=batch_size, verbose=0)\n",
    "\n",
    "print('Final t_OBS = '+format(-2*score,'1.5f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
